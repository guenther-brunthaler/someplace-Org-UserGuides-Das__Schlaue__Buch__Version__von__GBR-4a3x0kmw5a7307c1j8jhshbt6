00000Example-Model-Name/YYYY-MM-DD/URL 00000Example-Model/YYYY2-MM2-DD2/URL2 ...: Review as of YYYY-MM-DD extracted from URL, augmented with additional information from another date/URL. URL and date is optional. Any number of review sources and model variants are allowed in the entry header (terminated by colon and SPACE). \n\n This is a new paragraph. Paragraphs are separated by the sequence SPACE Backslash n Backslash n SPACE. All entries should be sorted lexocographically using the collation sequence of the POSIX locale.
Flux.2/2025-12-20/https://heise.de/-11120129 WAN%202.22/025-12-20/https://heise.de/-11120129: Und es gibt natürlich auch Bild- und Videogenerierungsmodelle, die auch lokal laufen und die ziemlich gut sind und die man zum Beispiel auch selbst feintunen kann. Die man also selbst anpassen kann, dass da wirklich der Stil rauskommt, den man gerne haben will. Ganz aktuell ist da zum Beispiel Flux.2 für Bilder aus dem Schwarzwald oder WAN 2.2. für Videos.
Nvidia-Nemotron-3-Nano-30B-A3B/2025-12-19/https://heise.de/-11120636: Bei Nemotron 3 hat Nvidia die Modelle von Grund auf neu trainiert und sich dafür eine neue Architektur ausgedacht. Die bisherigen Mixture-of-Experts-Layer (MoE) verwendet Nvidia dabei abwechselnd mit Mamba-Layern, die im strengen Sinne keine Transformer-Architektur nutzen. Der Vorteil ist eine deutlich höhere Ausführungsgeschwindigkeit und ein geringerer Speicherverbrauch, weil der Key-Value-Cache, der sich den Kontext merkt, in den Mamba-Layern nicht mit der Kontextlänge wächst. Vermutlich genau aus diesem Grund konnte Nvidia die Kontextlänge auf eine Million Token erhöhen. Das Modell eignet sich somit für sehr lange Dokumente. \n\n Obwohl das Modell „Nano“ im Namen trägt, ist es nicht wirklich klein, sondern hat 31,6 Milliarden Parameter, von denen es bei jeder Token-Vorhersage 3,6 Milliarden verwendet. Das macht das Modell schnell, dazu tragen außerdem die leichter zu berechnenden Mamba-Layer bei. Nvidia spricht von einem Faktor 3,3 gegenüber vergleichbaren Modellen. \n\n Nemotron Nano besteht aus 52 Layern mit einer Modelldimension von 2.688 und setzt auf 32 Attention-Heads. Die Mamba-Layern haben 128 Zustandsdimensionen in acht Gruppen, mit jeweils 64 Mamba-Heads in 64 Head-Dimensionen. Insgesamt gibt es 128 Experten, von denen zwei als Shared Experts arbeiten; weiterhin aktiviert das Modell sechs zusätzliche Experten. Da die Experten nur 1.856 Dimensionen haben, erklärt sich so die Anzahl der aktiven Parameter von 3,6 Milliarden. \n\n Was das Nemotron-Modell allerdings wirklich gegenüber fast allen anderen Modellen auszeichnet, sind die Trainingsdaten. Nahezu alle hat Nvidia veröffentlicht, genau wie auch die im Training verwendeten Algorithmen. \n\n Insgesamt stehen 10 Billionen Tokens zum Download in den Datensets zur Verfügung. \n\n Für das Pre-Traning nutzt Nvidia den Warmup-Stable-Decay als Scheduler und trainiert das Modell mit 25 Billionen Token in 15 unterschiedlichen Kategorien. \n\n Das Pre-Training erfolgt in 19 Sprachen: Arabisch, Chinesisch (vermutlich Mandarin), Tschechisch, Dänisch, Flämisch, Finnisch, Französisch, Deutsch, Hebräisch, Hindi, Italienisch, Japanisch, Koreanisch, Portugiesisch, Polnisch, Russisch, Spanisch, Schwedisch und Thai. \n\n Am Ende quantisiert Nvidia das bfloat16 Modell noch auf FP8 und zeigt, dass damit fast keine Qualität verloren geht. Vermutlich hat man das auch mit NVFP4 probiert und dabei schlechtere Ergebnisse erzielt und daher die größeren Modelle gleich in diesem Datenformat trainiert.
