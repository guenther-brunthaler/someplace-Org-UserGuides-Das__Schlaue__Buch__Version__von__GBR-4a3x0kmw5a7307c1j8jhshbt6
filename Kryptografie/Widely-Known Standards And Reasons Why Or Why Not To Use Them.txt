* NIS
NIS has negligible security.
http://www.vanemery.com/DAS/110-introduction.html
In password change requests, the old password will be sent over the network in plaintext - even if the password change request fails and the old password therefore remains valid.

* NIS+
NIS+ is not widely supported outside of Solaris and is, in fact, being dropped by Sun. Linux support of NIS+ is not complete on the server side, and Sun appears to be abandoning NIS+ in favor of LDAP + Kerberos 5. Also, NIS+ has a bad reputation for manageability.
http://www.vanemery.com/DAS/110-introduction.html

* RADIUS
This is primarily used to control subscriber access to ISPs. Although there are PAM modules for Linux and *BSD that allow RADIUS authentication, RADIUS does nothing to supply required user and group information on the host. The encryption scheme is also not particularly strong, and there is a shared secret between clients and server.
http://www.vanemery.com/DAS/110-introduction.html

* NFSv2
"... NFSv2 only allowed the first 2 GB of a file to be read." In addition, it has at least the same security flaws as NFSv3 (see below).
http://en.wikipedia.org/wiki/NFSv4#NFSv4
Also, NFSv2 can only use UDP for transport, while NFSv3 can at least optionally enable TCP transport. While an advantage of UDP is the fact that it consumes very little resources on the server, it has the severe disadvantage that NFSv2 protocol units cannot be larger than the network's MTU allows, which is frequently around 1500 bytes only. This means, all read/write operations have to be chunked into 1500 byte units, while there is no such restriction in TCP (TCP too will have to segment the transported data into single IP packets, but this is done transparently on the network layer which has much lower overhead than NFS at the application layer). Also, TCP is more reliable in case of transmission errors.

* NFSv3
The security of NFSv3 is based on the ridiculously naive assumption that the client machine never lies about the identity of the client who is trying to connect. This would require all NFSv3 clients which can reach the server to be under the same strict administrative control, and that no local users can get superuser rights or can boot a different operating system where they have such rights. Otherwise, the client machine can tell the server anything about who is trying to connect, and the NFSv3 server will believe it blindly. Therefore, NFSv3 is practically useful only in strictly isolated, physically access-restricted LANs where only known and trusted machines are operating, or for read-only access to public, non-sensitive data. For any other applications which require any security at all, NFSv4 with Kerberos protection should be used rather than NFSv3. Another problem of NFSv3 is how it handles files owned by "root". A connecting user "root" (on the client) will automatically be mapped to user "nobody" on the server, which means "root" has not more but even less rights there than normal users. However, this helps little in practice, because user "root" can assume the identity of any other user on the client and connect to the NFS server as that user. The next problem is that NFSv3 does not map user/group IDs: If uid 1014 is user "joe" on the server and user "mary" on the client, mary will have full access to user joe's files when connecting via NFSv3. In fact, when a writable NFS share is exported, remote clients can write to *every* file and directory on the server, except those files and directories restricted to user root (on the server machine). This is why every important executable should always be owned by user "root", not by "adm", "sys", "daemon" or whatever. Because NFS users could impersonate all those users as they connect to the server, also gaining write access as those users on the server unless the NFS share is exported read-only. In other words, the security of NFSv3 is near to zero. All its security mechanisms are extremely weak bordering to bogus, there is no encryption or authentication whatsoever. The only reason to set up an NFSv3 server might be for easy accessing shared files, like a public ftp server. But even then there remains the danger that all files from the server can be read which are not owned by root, exposing (if /home is exported) the most private or secret files of all local users on the server for free read-access to everyone on any client machine which is allowed connect to the server at all, bypassing the whole UNIX access rights mechanism. On the other hand, exporting only NFS shares with no sensitive data at all on them might be tolerable.

* FIPS 140
"Because FIPS 140 has certain restrictions on the use of cryptography which are not always wanted, Libgcrypt needs to be put into FIPS mode explicitly"...

* SNMP
Versions 1 and most Revisions of version 2 have no real security at all; they authenticate using simply plain text messages sent over the network unencrypted. Version 3 adds cryptographic security, but does not properly protect against brute-force and dictionary attacks because no Challenge/Response or salts are used. Depending on the specific algorithms used and supported by an implementation, the entropy of session keys can also be an issue. It might not be impossible to create a rather secure setup with SNMPv3, but it can be hard to establish and the firmware of many network devices only implements suboptimal configurations.

* S/KEY
S/KEY generates a sequence of one-time-passwords by concatenating a non-secret salt with a secret password, and applying a hash function recursively for N iterations. The Nth hash is saved, but the (N-1)th hash value is asked for as a password. Therefore, the server has never to store the actual password, only the result of hashing it. When the next password (the hash value from the previous iteration) is entered, it is hashed in order to see whether it matches the stored hash. If it does, the password is value and the password hash just entered is stored in the database, requiring to enter the (N-2)th hash for the next login. The basic idea of S/KEY is therefore quite good. But in practice it has a severe design flaw: No matter how large the output of the hash function used may be, it is "folded" (= reduced) to 64 Bit in each iteration. This means that an attacker who can sniff one of the one-time-passwords, can find the next valid password with a brute-force offline attack of complexity 2 ** 63 on the average, by just hashing every possible 64-bit-value, "folding" the result, and compare it with the sniffed one-time-password. An offline-attack of order 2 ** 64 is not too difficult to perform, even for moderately equipped adverseries. Another problem of S/KEY is that it cannot protect against man-in-the-middle-attacks. Therefore, S/KEY should not be used at all.

* HOTP with only a user-provided PIN
HOTP is an HMAC-based one-time password algorithm. It is used for two-factor authentication and is based on a cryptographically strong hash function, a counter and a secret key. The counter is 64 bit and the secret key must be provided by the user on the cryptographic token device in order to calculate the next "one-time password". The problem of this scheme is the small counter size in combination with a potentially weak user-provided key. If an attacker can guess the counter value, or at least limit the probable range of current counter values, an offline brute force attack on the user-provided password is possible. But even if the initial value of the counter is chosen at random, this fact only increases the total effort for the brute-force attack by 64 bits. As an example, let us assume a cryptographic token device has only numeric input keys and requires a 10-digit PIN. The PIN then has a strength of about 33 bits, and the unknown counter value adds another 64 bits of strength. This gives a total strength of about 97 bits. If the PIN was only 6 digits, the effort for the attacker would be 84 bit. If the PIN was only 4 digits, the effort would be 77 bits. Those strengths might not be large enough for highly security-relevant applications. The truncation of the displayed result on the token device makes a brute force attack against a single observed OTP value infeasible. But this can easily resolved by verifying the brute force result against a small number of observed OTP values actually sent by the client to the server, which eliminates the remaining abiguity. However, HTOP could be made sufficiently safe with a simple modification: It would be sufficient to increase the size of the counter to at least 128 bits, and *require* the counter to be initialized to a random number. However, the HTOP specification leaves another possiblity open for strengthening, although it is optional an therefore not required: Instead of using a stronger counter, it is also possible to use a stronger key by using a "composite shared secret", where the user only provides part of the secret, and an additional fixed key of suitable strength is shared between server and token device. If such an optional additional shared key of 128 bit or more is actually used, HTOP should be safe enough for recommended use. Equivalently, the token device might use an internally-stored key of much larger size than the PIN, and the PIN is just used as a password to decrypt the key. However, this does not make the resulting system any stronger than a simple composite shared key extension.

* TOTP with only a user-provided PIN
TOTP is basically the same as HTOP with the counter replaced by a value derived from current date/time. It therefore has the same security weaknesses and should not be used except in a configuration with a composite shared secret of at least 128 bit in addition to any user-provided PIN. Actually, the effectiveness of the PIN is limited unless the device is really tamper-proof when stolen.

* Socialist millionaire Cryptographic Protocol
This protocol makes complicated use of multiple interwoven Diffie-Hellman exchanges in order to prove that X = Y between two parties having X and Y but not wishing to reveal the actual values. It is claimed that this protocol can avoid the requirement of fingerprint comparisons between X and Y. I doubt this severely. First, IMHO this protocol is unnecessarily complicated. I could establish the same effect by just exchanging a single shared Diffie-Hellman-Secret in the first step, then let party 1 calculate A=hash(shared_secret, X) and party 2 calculates B=hash(shared_secret, Y). If A=B then X=Y. In order to exchange A and B securely, they can be sent xored with a second Diffie-Hellman shared secret. However, I do not see whether this or the original protocol should prevent a MITM attack. The protocol could indeed verify that the person I am communicating with is the same person who owns the public key I downloaded from somewhere in the net. But this is no proof that this person is actually the person I think! Anyone could have created a public key claiming to be that person and upload the key to the server where I downloaded it. Without personal first-hand proof which means exchanging fingerprints on a person-by-person meeting no-one can be sure about the other's identity. Of course, pyramid-of-trust-scenarios are always possible, but in any case it is necessary to compare fingerprints at least at one point. Therefore, I cannot see the actual benefits of the above protocol - IMO, it establishes nothing in addition to that which other cryptographic algorithms already provide. And certainly it cannot avoid the requirement for fingerprint comparisons.

* PXE
To quote from [ http://www.scriptjunkie.us/2011/08/network-nightmare/ ]: As a quick summary, the Preboot Execution Environment, available on almost all motherboards as “Network Boot,” provides a way for anyone who can run a DHCP server on the subnet to take complete control of the booting system before the hard drive is ever accessed. We can use pxelinux, a linux bootloader for PXE, to load up a linux kernel and initrd into the memory of the booting system for complete control. This may include shellcode that will be run online or dropped onto the hard disk and run on boot in the operating system, and is now available as the pxesploit modules in Metasploit, providing a variety of attacks for direct attack or pivoting via meterpreter. Securing PXE is difficult, so the best idea is probably to simply turn the feature off.

* WEP
To quote from [ http://en.wikipedia.org/wiki/WPA2 ]: [...] the amendment deprecated broken Wired Equivalent Privacy (WEP) [...]. And to quote from [ http://en.wikipedia.org/wiki/CCMP ]: [...] was created to address the vulnerabilities presented by WEP, a dated, insecure protocol.

* TKIP
Temporal Key Integrity Protocol [...] was [...] an interim solution to replace WEP without requiring the replacement of legacy hardware [...] TKIP is no longer considered secure and was deprecated [...]

* WPS
* Wi-Fi Protected Setup
To quote from [ http://en.wikipedia.org/wiki/WPA2 ]: A major security flaw was revealed in December 2011 that affects wireless routers with the Wi-Fi Protected Setup (WPS) feature, which most recent models have and enable by default. The flaw allows a remote attacker to recover the WPS PIN and, with it, the router's WPA2 password in a few hours [...] And to quote from [ http://en.wikipedia.org/wiki/Wi-Fi_Protected_Setup ]: It is highly recommended that wireless access points which are factory secured and/or supporting Wi-Fi Protected Setup be kept in a physically secure area, preferably under video surveillance.

* SSL-3.0
Entdeckt wurde die POODLE (Padding Oracle on Downgraded Legacy Encryption) getaufte Lücke von drei Sicherheitsforschern bei Google. Ursache ist das veraltete SSL-3.0-Protokoll. Die Google-Forscher haben einen Weg gefunden, wie dieses ausgetrickst werden kann, um an sensible Daten zu kommen [...] Die POODLE-Attacke lässt sich zudem nicht einfach durch ein Update beheben, handelt es sich dabei doch um eine prinzipielle Schwäche im SSL-3.0-Protokoll. Stattdessen raten die Sicherheitsforscher zur vollständigen Abkehr von der veralteten Verschlüsselungsmethode. Als ersten Schritt sollten Webseiten-Administratoren die SSL-3.0-Unterstützung auf ihren Servern deaktivieren. Zudem empfiehlt man auch allen Browserherstellen, den Support für dieses Protokoll einzustellen.

* UAF / U2F
Diese Standards sollen helfen, Tokens wie den Yubikey in eine Sicherheitsarchitektur zu integrieren. Sie ist zwar grundsätzlich offen, doch wird schnell klar dass der Hersteller des Yubikey einer der Hauptproponenten ist. Was mich daran stört ist die riesigen Komplexität des Standards, welche über bereits bestehende und ohnehin bereits viel zu komplizierte Standards wie TLS und X.509 noch drüber gestülpt wird: Etwas so komplexes wird immer so viele Implementierungsfehler enthalten, dass es niemals wirklich sicher sein kann. Weiters stören mich wertende Aussagen in den Standards, wo 3DES allgemein als "schwach" und AES als "stark" beschrieben wird. Außerdem bringt der Standard unnotwendiger Weise asymmetrische Verschlüsselung ins Spiel, obwohl man gerade Tokens auch sehr gut mit wesentlich zuverlässigerer symmetrischer Verschlüsselung kombinieren könnte. Im Falle von U2F kommt noch hinzu, dass dieses einen "herstellerabhängigen Private Key" enthalten, der "für alle Geräte dieses Herstellers identisch ist" und "kann Dienstanbietern dazu dienen, für den Zugang die Verwendung bestimmter U2F-Geräte einzufordern". Was soll diese Frechheit? Einen offenen Standard sollte man unabhängig von jedem Gerätehersteller implementieren können, ohne dass Webseiten erkennen können was für eine Implementation man verwendet, geschweige denn dies Einschränken zu können! Fazit: Diese Standards sind offensichtlich nur dazu gedacht dass es zu weniger Betrügereien im im B2C-Geschäft kommt, aber sind kein ausreichender Schutz gegen potente Angreifer wie Geheimdienste und Hacker, welche Implementations- und Sicherheitslücken gnadenlos ausnutzen. Insgesamt scheint es somit ählich "sicher" zu sein wie TLS - also nichts, dem ich mein Leben anvertrauen wollte.

* EXT4 Built-In Encryption
Funktioniert auf Directory-Ebene und hat gute Performance, da es direkt im Dateisystem integriert ist. Konkret soll es angeblich eine bessere Performance und geringeren Speicherverbrauch als EcryptFS und auch LUKS/DMcrypt haben. Nachteil ist, dass zur Zeit (2015-06-23) nur Verschlüsselung, jedoch keinerlei Integritätskontrolle (MAC) vorgesehen ist. Diesbezüglich bietet beispielsweise EncFS (optional) mehr. Ebenso kann nur AES-256 zur Verschlüsselung verwendet werden, wobei der XTS-Modus für Dateiinhalte und der CBC-CTS-Modus für Dateinamen und Symlinks verwendet wird. Zur Ableitung diverser Zwischenschlüssel und Hash-IDs kommen SHA2-256 und SHA2-512 zum Einsatz. Weiters wird ein Nonce pro Datei in Extended Attributes abgelegt; das stellt höhere Ansprüche an Backups (da EAs mitgesichert werden müssen) und erfordert dass EA-Support für das Dateisystem aktiviert sind. Metadaten wie Dateilänge werden weder geschützt oder verborgen. Dateien können ohne Schlüssel zwar nicht geöffnet werden, wohl aber gelöscht (normale Löschrechte aufs Verzeichnis voraus gesetzt). Der erforderliche Master-Key wird aus dem Kernel-Keyring für "Logon"-Keys genommen. Ein praktischer Nachteil für die nächste Zeit stellt die Neuheit dieser Implementation dar; nur die allerneuesten Kernel-Versionen werden sie bereit stellen. Fazit: Da man derzeit auf den wenig vertrauenswürdigen AES-Algorithmus beschränkt ist, ist die Sicherheit dieser Lösung eher nur im Mittelfeld zu sehen. Der einzige wirkliche Grund der dafür spricht ist die Performance. Auch wenn ich gewisse Zweifel habe, dass diese Lösung wirklich performanter als DMcrypt mit AES-XTC ist.

* EcryptFS
Funktioniert ähnlich wie die an anderer Stelle erläuterte EXT4-Verschlüsselung, bietet aber etwas mehr Auswahl bei den Verschlüsselungsmethoden. Die einzigen darunter denen ich halbwegs traue sind allerdings auch nur Blowfish und TripleDES; konkret fehlt leider SERPENT. Der Overhead soll wesentlich höher als der von EXT4-Encryption sein. Ein Vorteil ist hingegen, dass optional eine HMAC-Integritätskontrolle durchgeführt werden kann. Ein klarer Nachteil ist der derzeit (2015-06) nach wie vor herrschende experimentelle Status dieses Overlay-Dateisystems, so dass sich das Metadaten-Layout der verschlüsselten Daten theoretisch zwischen jeder Kernel-Version ändern können, und die Daten danach mit den neuen Kernel unleserlich wären. Weiters missfällt mir die vergleichsweise große Anzahl an Utility-Programmen, welche offenbar zum Management dieser Lösung vonnöten sind. EncFS bietet das alles mit nur 2 Utilities. Allerdings dürfte EcryptFS wohl sicherer als das von argen Design-Schwächen geplagte EncFS sein. Gleichwohl ist EcryptFS wegen der Gefahr jederzeitiger Änderungen im On-Disk-Layout schlecht geeignet für Langzeit-Archivierung oder Datenaustausch mit anderen Systemen welche andere Kernel-Versionen verwenden mögen.

* Scrypt
Ein auf HMAC-AES-256 basierendes Key-Stretching-Verfahren, das so konfiguriert werden kann dass es sehr viel RAM benötigt, und sich daher schlecht mittels FPGAs und ASICs beschleunigen lässt. Das Verfahren mag nett und gut sein, aber es ist nur für Schlüssel bis 256 Bit Schlüsselstärke geeignet, weil längere Schlüssel von diesem "Stretching"-Verfahren (welch ein Hohn), effektiv auf HMAC auf 256 Bit heruntergestutzt werden! Andererseits sind 256 Bit vermutlich sicher genug, selbst wenn der Angreifer sie mittels zukünftiger Quantencomputer auf effektive 128 Bit effektiver Schlüsselstärke reduzieren kann. Aber 128 Bit sollten immer noch sicher genug für die nächsten 100 Jahre oder so sein, so dass wenig Gefahr besteht dass solche Schlüssel noch zur eigenen Lebenszeit geknackt werden. Will man allerdings auch für die langzeit-vorhersehbare Zukunft gewappnet sein, wäre ein 512 Bit-Schlüssel erforderliche, den man mit Scrypt nicht realisieren kann. Ansonsten kann ich auf den ersten Blick keine offensichtlichen Schwächen von Scrypt entdecken; es dürfte zumindest auf jeden Fall besser sein als PBKDF2 allein (welches es intern als Teil seiner Berechnungen ebenfalls verwendet).

* WebAuthn
Ein vom W3C standardisiertes Anmeldeverfahren, an dem die Gestalter von U2F und UAF (die FIDO Alliance) maßgeblich mitgewirkt zu haben scheinen. Es gibt 3 Beteiligte: Den Service-Provider zu dessen Diensten Zugang erlangt werden soll, ein JavaScript das im Browser läuft, und einen "Authenticator" der offenbar zumindest von Client aus über das Netzwerk erreichbar sein muss (etwa auf 127.0.0.1 an einem Port lauschend) und entweder einen Software-Dienst oder ein Hardware-Security-Token wie der YubiKey zu sein scheint. Das eigentliche Verfahren sendet dann Public Keys und digitale Signaturen hin und her und dürfte eher ein geringeres Problem sein. Problem 1 das ich sehe: JavaScript ist bei dem Verfahren zwingend nötig. Problem 2: Wer startet den "Authenticator"-Service, auf welchem Gerät, und wie lange muss er laufen? Weiters finden sich in der Wikipedia Kritikpunkte an den kryptografischen Verfahren, die angeblich geschwächt sind im Vergleich dazu was möglich wäre - aber auch wieder nicht so sehr dass dies für einen Angriff ausreichen würden. RSA scheint zwingender Bestandteil des Standards zu sein, aber es gibt viele Optionen. Das sehe ich ebenfalls als Problem, denn wenn eine bestimmte Implementation einzelne Optionen nicht implementiert, was dann?

* X9.31 CSRNG
Voller Titel: "NIST-Recommended Random Number Generator Based on ANSI X9.31 Appendix A.2.4 Using the 3-Key Triple DES and AES Algorithms".
Hat mir zwar ein bisschen zu viel administrativen Overhead, scheint mir im Kern aber ausreichend sicher zu sein. Allerdings liefert er nur eine Qualität an Zufallszahlen die mit der von /dev/urandom vergleichbar ist, während er nur für eine Minderzahl an generierten Zufallsbytes die Qualität wie /dev/random erreicht. Tatsächlich kann die Qualität sogar geringer als die von /dev/urandom sein. Denn /dev/urandom wird von den Entropiequellen ständig gespeist auch wenn keine Garantie besteht dass genug Entropie vorhanden ist. X9.31 wird hingegen nur in periodischen Abständen "reseeded", und nur bei dieser Gelegenheit fließt Entropie aus hochwertigen Quellen in den Generator ein. Falls der Generator hingegen ständig arbeitet, kann die Qualität geringfügig höher als die von /dev/urandom sein, da dann zumindest bei den gelegentlichen Reseeding-Operationen sichergestellt ist, dass hochwertige Entropiequellen angezapft werden. Ein Nachteil von X9.31 ist der relativ hohe Kernel-Overhead falls die Funktion gettimeofday() teuer implementiert sein sollte (in Linux ist dies meines Wissens nicht der Fall), da der Generator sie für jeden 128-Bit Block an Ausgabedaten aufruft. Zudem wird zumindest in der libgcrypt-Implementation für jeden Aufruf des Generators immer ein neuer 128-Bit-Block erzeugt, selbst wenn weniger Zufallsbytes angefordert wurden. Dadurch wird gettimeofday() im schlimmsten Fall für jedes Zufalls-Byte aufgerufen, und dazu noch 3 Blockverschlüsselungen durchgeführt. Grundsätzlich wird der Generator immer direkt oder indirekt durch /dev/random initialisiert, so dass zumindest in dieser Hinsicht nichts zu befürchten ist wenn man dem Kernel-Zufallsgenerator trauen kann. Der X9.32-Algorithmus basiert im wesentlichen auf dem XEX-Prinzip (XOR-Encrypt-XOR), das als sehr sicher gilt. Dabei wird vom Seed ausgehend das XE-Zwischenergebnis als neue Zufallsbytes geliefert, während das komplette XEX den neuen Seed ergibt. Der XOR-Wert wird für jeden neuen Zufallsblock neu erzeugt, indem ein Datenblock verschlüsselt wird, welcher sowohl den oben erwähnten gettimeofday()-Wert mit Mikrosekundenauflösung als auch einen fortlaufenden Zähler enthält. Allein durch den Zähler sollte ausreichend sicher gestellt sein, dass der XOR-Wert sich nie wiederholt - zumindest solange der Schlüssel gleich bleibt. Dieser wird allerdings nach einer gewissen Anzahl (1000 bei libgcrypt) an erzeugten 128-Bit Blocks "reseeded", wozu entweder direkt /dev/random verwendet wird, oder ein anderer Zufallsgenerator, der seinerseits von /dev/random initialisiert wurde. Die libgcrypt-Implementierung initialisiert darüber hinaus den oben erwähnten fortlaufenden Zähler nicht durch einen fixen Wert, sondern durch die process ID des aufrufenden Prozesses sowie seines Vaterprozesses. Zwar sind weder process IDs noch gettimeofday() etwas für einen Angreifer grundsätzlich schwer einzugrenzendes, aber das sollte hier kein Problem sein da der fortlaufende Zähler selbst bei konstantem Initialisierungswert meiner Ansicht nach sicher genug wäre. Letztendlich hängt die Sicherheit des Generators nur an der Verschlüsselungsfunktion und der Unvorhersagbarbeit von /dev/random, wobei die XEX-Konstruktion selbst Schwächen in der Verschlüsselungsfunktion weniger problematisch macht. Das fortwährende Integrieren der Tageszeit mit Mikrosekundenauflösung in den Generationsprozess erhöht zwar bei den ersten Zufallszahlen kaum die Sicherheit, doch je länger der Generator läuft desto schwieriger wird es für den Angreifer nachzuvollziehen welche exakten Timestamps in den Generationsprozess eingegangen sind. Fazit: Mir scheint dieser Generator sicher zu sein und auf einer soliden Grundidee zu basieren. Allerdings würde ich ihn aus Performance-Gründen nicht ohne Modifikationen verwenden wollen. Ich würde dabei die gettimeofday()-Funktion seltener aufrufen, und beim Reseeding den Seed nicht komplett durch einen neuen ersetzen, sondern einen solchen mit dem vorherigen Zustand des Generators kombinieren. Ein Nachteil von X9.31 ist dass es sich im Grunde nicht um einen vollwertigen CSRNG handelt, sondern eher um ein "Strecken" der Zufallszahlen welche /dev/random liefert. Jeder generierte 128-Bit Block an Zufallszahlen entzieht dem Zustand des Generators 128 Bits an Entropie, während nur maximal 36 Bit durch die Tageszeit hinzu kommen. Und selbst das nur wenn der Angreifer nicht eingrenzen kann wann der Generator aufgerufen wurde. Nach 3-4 Ausgabe-Blocks ist daher die Entropie des Generators aufgebraucht, und der Rest bis zum nächsten Reseeding (das aber im Fall von libgcrypt erst rund 1000 Ausagabeblocks später erfolgt) sind nur noch "minderwertige" Zufallszahlen wie man sie auch von /dev/urandom bekommt.

* ZIP file encryption
The InfoZIP standard defines 3 encryption variants: AE-1, AE-2 and a legacy encryption mode. The legacy mode is cryptographically weak and should not be used. AE-1 and AE-2 both use AES encryption, but AE-1 has weaker authentication and should therefore also be avoided. AE-2 uses an 80-bit MAC for authentication; actually it is a shortened HMAC-SHA1. The MAC is calculated after encryption (Encrypt-then-MAC), which is generally recommended by cryptographers. ZIP supports 3 key sizes for encryption: 128, 192 and 256 bit. The encryption uses AES in counter mode and a salt (at least 8 bytes for 128 bit keys; longer salts for longer key sizes). The encryption key is derived from the password via PBKDF2-HMAC-SHA1. This means that the derived encryption key will not contain more than 160 bit of entropy, making 192 bit keys partially and 256 bit keys completely useless. It is therefore recommended to use 128 bit keys only, or 192 bit keys which actually are only 160 bits strong. Encryption in ZIP-files is per-entry: Every entry has an individual encryption header, and can use a different encryption variant, different password, or no encryption at all. Normally, all entries in a ZIP archive use the same encryption method and password, but this is not a requirement. Assessment: ZIP encryption is sufficiently secure when AES-128 and AE-2 is used, provided one trusts AES at all. AES-256 is pointless because the key derivation can only retain up to 160 bit of password entropy. AES-192 *may* be used but it is a partial waste because in that case 32 of the key bits are unused/constant. There is a danger that encrypted files from an archive will be replaced by unencrypted fake files by an attacker; the receiver will usually not note this unless he examines the encryption status of every file in the archive prior to extraction, which is not common practice. Another problem is that the file names in a ZIP file are not encrypted. However, double-ZIP-encryption can be used to solve both problems: First create an unencrypted inner ZIP file containing the secret files. Then create an encrypted outer ZIP file, containing only a single file entry: The inner ZIP file. That way, the file names within the inner ZIP file remain secret, and as the outer ZIP file only contains a single encrypted file, there is no way an attacker can replace it by an unencrypted file without the user noticing (because no password will be asked for). Examples for implementations: The Android version of Total Commander contains a ZIP packer which creates AE-2 encrypted AES-128 ZIP entries.
